{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 構成\n",
    "|-- *activation_funcs.py ：アクティベート関数(sigmoid関数 / softmax関数)*<br>\n",
    "|-- *error_funcs.py ： エラー関数(二乗誤差関数 / 交差エントロピー関数)*<br>\n",
    "|-- *fonc.py：呼び出し名を用意させる*<br>\n",
    "|-- *layers.py ：１層、２層の層部分を構成*<br>\n",
    "|-- *networks.py：全体を構成*<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Func\n",
    "ネットワーク構成において名前で呼び出せるように親クラスを用意しておく"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Function:\n",
    "    name = ' '\n",
    "    \n",
    "    def get_value(self, *args, **kwargs):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def get_derivative(self, *args, **kwargs):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error_funcs\n",
    "エラー関数を用意<br>\n",
    "*二乗誤差関数　と　交差エントロピー関数がある*\n",
    "- get_value()：Fowardの通常出力\n",
    "- get_derivative()：関数の微分に入力値を与えて微分値を取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# from nn.func import Function\n",
    "\n",
    "class SquaredError(Function):\n",
    "    \"\"\"二乗誤差関数\"\"\"\n",
    "    name = \"se\"\n",
    "    \n",
    "    def get_value(self, t, y):\n",
    "        \"\"\"通常出力\"\"\"\n",
    "        return ((t - y).T @ (t - y)).ravel()[0] / 2.0  # flatten()よりravel()がいい\n",
    "   \n",
    "    def get_derivative(self, t, y):\n",
    "        return -(t - y)\n",
    "    \n",
    "    \n",
    "class CrossEntropy(Function):\n",
    "    \"\"\"交差エントロピー関数\"\"\"\n",
    "    name = \"cross_entropy\"\n",
    "    \n",
    "    def get_value(self, t, y):\n",
    "        return -(t.T @np.log(y)).ravel()[0]\n",
    "    \n",
    "    def get_derivative(self, t, y):\n",
    "        return - t/y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation_funcs\n",
    "アクティベート関数<br>\n",
    "*シグモイド関数　と　softmax関数が今回使用される*\n",
    "- get_value()：Fowardの通常出力\n",
    "- get_derivative()：関数の微分に入力値を与えて微分値を取得\n",
    "\n",
    "assert ：条件式, 条件式がFalseの場合に出力するメッセージ\n",
    "\n",
    "条件式が False の場合、AssertionError の例外が発生します。<br>\n",
    "条件式が True の場合は何も起こりません。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "# from nn.func import Function\n",
    "\n",
    "\n",
    "class Logistic(Function):\n",
    "    \"\"\"シグモイド関数\"\"\"\n",
    "    name = 'logistic'\n",
    "    \n",
    "    def get_value(self, s):\n",
    "        return 1 / (1 + np.exp(-s))\n",
    "    \n",
    "    def get_derivative(self, y):\n",
    "        assert isinstance(y, np.ndarray) and y.shape[1] == 1\n",
    "        jacobian = np.zeros(shape=(y.shape[0], y.shape[0]))\n",
    "        jacobian[np.diag_indices(y.shape[0])] = (y * (1 - y)).ravel()\n",
    "        return jacobian\n",
    "    \n",
    "\n",
    "class Tanh(Function):\n",
    "    name = 'tanh'\n",
    "    \n",
    "    def __init__(self, alpha, beta):\n",
    "        self._alpha = alpha\n",
    "        self._beta = beta\n",
    "        \n",
    "    @property\n",
    "    def alpha(self):\n",
    "        return self._alpha\n",
    "    \n",
    "    @property\n",
    "    def beta(self):\n",
    "        return self._beta\n",
    "    \n",
    "    def get_value(self, s):\n",
    "        return self._alpha * np.tanh(self._beta * s)\n",
    "    \n",
    "    def get_derivative(self, y):\n",
    "        assert isinstance(y, np.ndarray) and y.shape[1] == 1\n",
    "        derivative = np.zeros(shape=(y.shape[0], y.shape[0]))\n",
    "        derivative[np.diag_indices(y.shape[0])] = self._alpha * self._beta * \\\n",
    "        (1 - np.power(np.tanh(self._beta * y.flatten()), 2))\n",
    "        return derivative\n",
    "    \n",
    "    \n",
    "class Softmax(Function):\n",
    "    \"\"\"softmax関数\"\"\"\n",
    "    name = 'softmax'\n",
    "    \n",
    "    def get_value(self, s):\n",
    "        _s = s - np.max(s)\n",
    "        exp_s = np.exp(_s)\n",
    "        value = exp_s / np.sum(exp_s)\n",
    "        value[value < 10e-7] = 10e-7   # 小さすぎる場合10e-7で切ってる\n",
    "        value[value > 10e+7] = 10e+7 # 大きすぎる場合10e-7で切ってる\n",
    "        return value / np.sum(value)\n",
    "    \n",
    "    def get_derivative(self, y):\n",
    "        assert isinstance(y, np.ndarray) and y.shape[1] == 1\n",
    "        y_diag = np.zeros(shape=(y.shape[0], y.shape[0]))\n",
    "        np.fill_diagonal(y_diag, y.flatten())\n",
    "        return y_diag - y @ y.T\n",
    "    \n",
    "    \n",
    "class Rectifier(Function):\n",
    "    def get_value(self, s):\n",
    "        val = deepcopy(s)\n",
    "        val[val < 0.0] = 0.0\n",
    "        return val\n",
    "\n",
    "    def get_derivative(self, y):\n",
    "        val = deepcopy(y)\n",
    "        val[val > 0.0] = 1.0\n",
    "        val[val <= 0.0] = 0.0\n",
    "        y_diag = np.zeros(shape=(y.shape[0], y.shape[0]))\n",
    "        np.fill_diagonal(y_diag, val.flatten())\n",
    "        return y_diag   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layers\n",
    "１層、２層など各層をレイヤーとして層の中身を表記"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# from nn.activation_funcs import Logistic, Tanh, Softmax, Rectifier\n",
    "\n",
    "\n",
    "class BaseLayer:\n",
    "    name = 'base'\n",
    "\n",
    "    def __init__(self, n_output, n_prev_output, f):\n",
    "        \"\"\"\n",
    "        :param n_output: Number of this layer's output\n",
    "        :param n_prev_output: Number of previous layer's output\n",
    "        :param f: Activation function (callable)\n",
    "        :param df: Derivative of the activation function (callable)\n",
    "        \"\"\"\n",
    "        self._W = self._init_W(n_output, n_prev_output)\n",
    "        self._b = self._init_b(n_output)\n",
    "        self._f = f\n",
    "        self._y = None\n",
    "        self._delta = None\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.name\n",
    "\n",
    "    def _init_W(self, n_output, n_prev_output, **kwargs):\n",
    "        \"\"\"\n",
    "        Way of initializing weight matrix W\n",
    "        :param n_output: Number of this layer's output\n",
    "        :param n_prev_output: Number of previous layer's output\n",
    "        :param kwargs:\n",
    "        :return: numpy ndarray object having dimension n_output * n_prev_output\n",
    "        \"\"\"\n",
    "        return np.random.uniform(-1, 1, size=(n_output, n_prev_output))\n",
    "\n",
    "    def _init_b(self, n_output, **kwargs):\n",
    "        \"\"\"\n",
    "        Way of initializing bias\n",
    "        :param n_output: Number of this layer's output\n",
    "        :param kwargs:\n",
    "        :return: numpy ndarray object having dimension n_output * 1\n",
    "        \"\"\"\n",
    "        return np.random.uniform(-1, 1, size=(n_output, 1))\n",
    "\n",
    "    @property\n",
    "    def n_output(self):\n",
    "        return self._W.shape[0]\n",
    "\n",
    "    @property\n",
    "    def W(self):\n",
    "        return self._W\n",
    "\n",
    "    @W.setter\n",
    "    def W(self, value):\n",
    "        self._W = value\n",
    "\n",
    "    @property\n",
    "    def b(self):\n",
    "        return self._b\n",
    "\n",
    "    @b.setter\n",
    "    def b(self, value):\n",
    "        self._b = value\n",
    "\n",
    "    @property\n",
    "    def ave_abs_W(self):\n",
    "        return np.average(np.abs(self._W))\n",
    "\n",
    "    @property\n",
    "    def ave_W(self):\n",
    "        return np.average(self._W)\n",
    "\n",
    "    @property\n",
    "    def y(self):\n",
    "        return self._y\n",
    "\n",
    "    @property\n",
    "    def delta(self):\n",
    "        return self._delta\n",
    "\n",
    "    def propagate_forward(self, x):\n",
    "        self._y = self._f.get_value(self._W @ x + self._b)\n",
    "        return self._y\n",
    "\n",
    "    def propagate_backward(self, next_delta, next_W):\n",
    "        if next_W is not None:\n",
    "            self._delta = self._f.get_derivative(self._y) @ next_W.T @ next_delta\n",
    "        else:\n",
    "            self._delta = self._f.get_derivative(self._y) @ next_delta\n",
    "        return self._delta\n",
    "\n",
    "    def update(self, prev_y, epsilon):       # 引数でもし前の入力値がない場合、初期入力値 x1,x2が入る\n",
    "        Delta_W = self._delta @ prev_y.T  # これはバックプロップの微分のみ\n",
    "        self._W -= epsilon * Delta_W          # パラメータ調整　W\n",
    "        self._b -= epsilon * self._delta        # パラメータ調整   b\n",
    "\n",
    "    def to_json(self):\n",
    "        return {'type': self.name, 'W': self._W.tolist(), 'b': self._b.tolist()}\n",
    "\n",
    "\n",
    "class LogisticLayer(BaseLayer):\n",
    "    name = 'logistic'\n",
    "\n",
    "    def __init__(self, n_output, n_prev_output):\n",
    "        super().__init__(n_output, n_prev_output, Logistic())\n",
    "\n",
    "\n",
    "class TanhLayer(BaseLayer):\n",
    "    name = 'tanh'\n",
    "\n",
    "    def __init__(self, n_output, n_prev_output, alpha, beta):\n",
    "        super().__init__(n_output, n_prev_output, Tanh(alpha, beta))\n",
    "\n",
    "    def to_json(self):\n",
    "        data = super(TanhLayer, self).to_json()\n",
    "        data['alpha'] = self._f.alpha\n",
    "        data['beta'] = self._f.beta\n",
    "        return data\n",
    "\n",
    "\n",
    "class SoftmaxLayer(BaseLayer):\n",
    "    name = 'softmax'\n",
    "\n",
    "    def __init__(self, n_output, n_prev_output):\n",
    "        super().__init__(n_output, n_prev_output, Softmax())\n",
    "\n",
    "\n",
    "class RectifierLayer(BaseLayer):\n",
    "    name = 'rectifier'\n",
    "\n",
    "    def __init__(self, n_output, n_prev_output):\n",
    "        super().__init__(n_output, n_prev_output, Rectifier())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Networks\n",
    "Neuron全体のネットワークをまとめたもの"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# from nn.error_funcs import SquaredError, CrossEntropy\n",
    "# from nn.layers import LogisticLayer, TanhLayer, SoftmaxLayer, RectifierLayer\n",
    "\n",
    "\n",
    "class LayerTypeDoesNotExist(KeyError):\n",
    "    def __init__(self, layer_classes):\n",
    "        self.message = 'Layer type must be %s or %s.' % (\n",
    "            ', '.join([cls.name for cls in layer_classes[:-1]]), layer_classes[-1].name)\n",
    "\n",
    "\n",
    "class ErrorFuncDoesNotExist(KeyError):\n",
    "    def __init__(self, error_funcs):\n",
    "        self.message = 'error_func_name must be %s or %s' % ( ', '.join([func.name for func in error_funcs[:-1]]), error_funcs[-1].name)\n",
    "\n",
    "\n",
    "class Network:\n",
    "    _LAYER_CLASSES = [\n",
    "        LogisticLayer,\n",
    "        TanhLayer,\n",
    "        SoftmaxLayer,\n",
    "        RectifierLayer,\n",
    "    ]\n",
    "\n",
    "    _ERROR_FUNCS = [\n",
    "        SquaredError(),\n",
    "        CrossEntropy(),\n",
    "    ]\n",
    "\n",
    "    def __init__(self, name, n_input, error_func_name, epsilon):\n",
    "        self._name = name\n",
    "        self._n_input = n_input\n",
    "        self._layers = []\n",
    "        self._error_func = self._get_error_func_by_name(error_func_name)\n",
    "        self._epsilon = epsilon\n",
    "\n",
    "    @property\n",
    "    def name(self):\n",
    "        return self._name\n",
    "\n",
    "    @property\n",
    "    def layers(self):\n",
    "        return self._layers\n",
    "\n",
    "    @property\n",
    "    def epsilon(self):\n",
    "        return self._epsilon\n",
    "\n",
    "    @epsilon.setter\n",
    "    def epsilon(self, value):\n",
    "        self._epsilon = value\n",
    "\n",
    "    @property\n",
    "    def error_func(self):\n",
    "        return self._error_func\n",
    "\n",
    "    @error_func.setter\n",
    "    def error_func(self, name):\n",
    "        self._error_func = self._get_error_func_by_name(name)\n",
    "\n",
    "    def _get_error_func_by_name(self, name):\n",
    "        for func in self._ERROR_FUNCS:\n",
    "            if func.name == name:\n",
    "                return func\n",
    "        raise ErrorFuncDoesNotExist(self._ERROR_FUNCS)\n",
    "\n",
    "    def add_layer(self, type, n_output, **kwargs):\n",
    "        def get_layer_cls(type):\n",
    "            for cls in self._LAYER_CLASSES:\n",
    "                if cls.name == type:\n",
    "                    return cls\n",
    "            raise LayerTypeDoesNotExist(self._LAYER_CLASSES)\n",
    "\n",
    "        n_prev_output = self._layers[-1].n_output if self._layers else self._n_input\n",
    "        layer = get_layer_cls(type)(n_output, n_prev_output, **kwargs)\n",
    "        self._layers.append(layer)\n",
    "\n",
    "    def propagate_forward(self, input_datum):\n",
    "        output = input_datum\n",
    "        for layer in self._layers:\n",
    "            output = layer.propagate_forward(output)\n",
    "        return output\n",
    "\n",
    "    def propagate_backward(self, input_datum, teaching_datum):\n",
    "        delta = self._error_func.get_derivative(teaching_datum, self.propagate_forward(input_datum))\n",
    "\n",
    "        next_layer = None\n",
    "        for layer in reversed(self._layers):\n",
    "            delta = layer.propagate_backward(delta, next_layer.W if next_layer is not None else None)\n",
    "            next_layer = layer\n",
    "\n",
    "    def update(self, input_datum):\n",
    "        prev_layer = None\n",
    "        for layer in self._layers:\n",
    "            layer.update(input_datum if prev_layer is None else prev_layer.y, self._epsilon)\n",
    "            prev_layer = layer\n",
    "\n",
    "    def to_json(self):\n",
    "        return {'meta': {'name': self.name,\n",
    "                         'n_input': self._n_input,\n",
    "                         'error_func': self._error_func.name,\n",
    "                         'epsilon': self.epsilon},\n",
    "                'layers': [layer.to_json() for layer in self._layers]}\n",
    "\n",
    "    @classmethod\n",
    "    def from_json(cls, json_data):\n",
    "        network = cls(json_data['meta']['name'], json_data['meta']['n_input'],\n",
    "                      json_data['meta']['error_func'], json_data['meta']['epsilon'])\n",
    "        for layer in json_data['layers']:\n",
    "            try:\n",
    "                type = layer.pop('type')\n",
    "            except KeyError:\n",
    "                raise LayerTypeDoesNotExist\n",
    "            n_output = len(layer['W'])\n",
    "            network.add_layer(type, n_output, **layer)\n",
    "        return network\n",
    "\n",
    "\n",
    "class Classifier(Network):\n",
    "    def get_class(self):\n",
    "        return np.argmin(np.abs(1.0 - self._layers[-1].y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 実行方法\n",
    "\n",
    "- Networkインスタンスを作成\n",
    "- インスタンス\n",
    "    - **def __init__(self, name, n_input, error_funcs_name, epsilon):**\n",
    "    - ネットワークの名前、inputの数(2行、3行など)、エラー関数名、epsilon値を指定\n",
    "    \n",
    "- 上記で箱が出来上がる\n",
    "- 次にレイヤー層を作る\n",
    "    - **def add_layer(self, type, n_output, ** kwargs):\n",
    "    \n",
    "    - レイヤーのタイプ(logistic, softmax)、outputの数(2行、3行など)\n",
    "\n",
    "- 最後に実行だけど前進・逆伝播\n",
    "    - **def propagete_forward(self, input_datum):**\n",
    "    - 前進でinputデータを与えて出力される\n",
    "    \n",
    "    - **def propagate_backward(self, input_datum, teaching_datum):**\n",
    "    - 逆伝播でinputデータ、正解データを与えて学習させる\n",
    "    \n",
    "    - **def update(self, input_datum):**\n",
    "    - パラメータのupdateで、inputデータを与えて更新させる\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_data = np.array([[0,],[0,]])\n",
    "# #                       [0,],[1,],\n",
    "# #                       [1,],[0,],\n",
    "# #                       [1,],[1,]]\n",
    "# output_data = [[0,],\n",
    "#                         [0,],\n",
    "#                         [0,],\n",
    "#                         [1,]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "and_gate = Network('and_gate', 2, 'se', 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "and_gate.add_layer('logistic', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.72076408]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "and_gate.propagate_forward([[0,],[0,]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "and_gate.error_func.get_value(np.array([[0, ], [0, ]]), [[0,], ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for i, o in zip(input_data, output_data):\n",
    "# for _ in range(10000):\n",
    "#     print(and_gate.propagate_forward([[0,],[0,]]))\n",
    "# #     print(and_gate.error_func.get_value(np.array([[0, ], [0, ]]), [[0,], ]))\n",
    "#     and_gate.propagate_backward(np.array([[1, ], [1, ]]), [[0,], ])\n",
    "#     and_gate.update(np.array([[0, ], [0, ]]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and_gate.propagate_forward([[0,],[0,]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and_gate.error_func.get_value(np.array([[0, ], [0, ]]), [[0,], ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and_gate.propagate_backward([[0, ], [0, ], ], [[0],])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and_gate.update(np.array([[0, ], [0, ]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and_gate.propagate_forward([[0, ], [0, ]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and_gate.error_func.get_value(np.array([[0, ], [0, ]]), [[0,], ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# コードの追い方\n",
    "- 気になるポイントを決めてそのmethodをしっかり読み込む\n",
    "- ある程度、名前をみて想像しながら何をしているのかを読み解く\n",
    "- さらに気になる先へ飛んでみる\n",
    "\n",
    "- 本などを読んで勉強するときは、Neuronなどでは自分で入力値を決めて手で計算してながらコードを追いかけてみる\n",
    "- するとアルゴリズムを理解しやすい"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### パラメータのバイアスも自動的に作っている\n",
    "    def _init_W(self, n_output, n_prev_output, **kwargs):\n",
    "        \"\"\"\n",
    "        Way of initializing weight matrix W\n",
    "        :param n_output: Number of this layer's output\n",
    "        :param n_prev_output: Number of previous layer's output\n",
    "        :param kwargs:\n",
    "        :return: numpy ndarray object having dimension n_output * n_prev_output\n",
    "        \"\"\"\n",
    "        return np.random.uniform(-1, 1, size=(n_output, n_prev_output))\n",
    "\n",
    "    def _init_b(self, n_output, **kwargs):\n",
    "        \"\"\"\n",
    "        Way of initializing bias\n",
    "        :param n_output: Number of this layer's output\n",
    "        :param kwargs:\n",
    "        :return: numpy ndarray object having dimension n_output * 1\n",
    "        \"\"\"\n",
    "        return np.random.uniform(-1, 1, size=(n_output, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### and_gate = Network('and_gate', 2, 'se', 0.01)\n",
    "### 最初のネットワーククラスをインスタンス化しているところ\n",
    "\n",
    "        入力値( 名前、入力数、エラー関数名、ハイパーパラメータ)を入れている\n",
    "        エラー関数のみ深くなっていて呼び出している\n",
    "        def __init__(self, name, n_input, error_func_name, epsilon):\n",
    "        self._name = name\n",
    "        self._n_input = n_input\n",
    "        self._layers = []\n",
    "        self._error_func = self._get_error_func_by_name(error_func_name)\n",
    "        self._epsilon = epsilon\n",
    "        \n",
    "        呼び出されている先では、定数のエラー関数名を順番にとってきてそのクラスを返している\n",
    "        _ERROR_FUNCS = [\n",
    "        SquaredError(),\n",
    "        CrossEntropy(),\n",
    "        ]\n",
    "        def _get_error_func_by_name(self, name):\n",
    "            for func in self._ERROR_FUNCS:\n",
    "                if func.name == name:\n",
    "                    return func\n",
    "            raise ErrorFuncDoesNotExist(self._ERROR_FUNCS)\n",
    "            \n",
    "        エラー関数がSquaredError()と()としているのは\n",
    "        呼び出した先で('エラー関数名')をnameへ代入している為\n",
    "        class SquaredError(Function):\n",
    "            \"\"\"二乗誤差関数\"\"\"\n",
    "            name = \"se\"\n",
    "            \n",
    "        なので初期値self._error_funcへはエラー関数のインスタンスを入れている\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ---------------------------\n",
    "### and_gate.add_layer('logistic', 1)\n",
    "### レイヤー名を指定して追加する\n",
    "\n",
    "### 結構複雑で、\n",
    "    _LAYER_CLASSES = [\n",
    "        LogisticLayer,\n",
    "        TanhLayer,\n",
    "        SoftmaxLayer,\n",
    "        RectifierLayer,\n",
    "    ]\n",
    "    レイヤー名は初期値では空の[]配列になっており\n",
    "\n",
    "    def __init__(self, name, n_input, error_func_name, epsilon):\n",
    "        self._name = name\n",
    "        self._n_input = n_input\n",
    "        self._layers = []\n",
    "        self._error_func = self._get_error_func_by_name(error_func_name)\n",
    "        self._epsilon = epsilon\n",
    "\n",
    "    addレイヤーされる時に\"logistic\"名をtype引数で受け取り\n",
    "    次の流れは以下のdef get_layer_cls()methodを飛び越えて\n",
    "    n_prev_outputへ\n",
    "    この中でself._layers[]リストに入っている一個前のレイヤー名を取り出し\n",
    "    layer = get_layer_cls(type)(n_output, n_prev_output, **kwargs)によって\n",
    "    まずget_layer_cls('logistic')で先ほど飛び越したメソッドへlogisticのクラス名でクラスを取得\n",
    "    そのクラス名へ(n_ouput, n_prev_output)初期値を与えてインスタンスを作成している 受けているのが\n",
    "    layer = である\n",
    "    class LogisticLayer(BaseLayer):\n",
    "    name = 'logistic'\n",
    "\n",
    "    def __init__(self, n_output, n_prev_output):\n",
    "        super().__init__(n_output, n_prev_output, Logistic())\n",
    "        \n",
    "    'logistic'名のクラス名は初期値を入れるようになっている\n",
    "    そのインスタンスをself._layers.append追加しておいてる\n",
    "    \n",
    "    def add_layer(self, type, n_output, **kwargs):\n",
    "        def get_layer_cls(type):\n",
    "            for cls in self._LAYER_CLASSES:\n",
    "                if cls.name == type:\n",
    "                    return cls\n",
    "            raise LayerTypeDoesNotExist(self._LAYER_CLASSES)\n",
    "\n",
    "        n_prev_output = self._layers[-1].n_output if self._layers else self._n_input\n",
    "        layer = get_layer_cls(type)(n_output, n_prev_output, **kwargs)\n",
    "        self._layers.append(layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -----------------\n",
    "### and_gate.propagate_forward([[0,],[0,]])\n",
    "### フォワードメソッドについて\n",
    "\n",
    "      メソッドを呼び出し、一旦入力値をoutput変数へ入れておき\n",
    "      add_layerで設定されているレイヤーインスタンスが入ったリストを順番に呼び出しいる\n",
    "      呼び出されたlayerインスタンス.propagate_forward(output)を呼び出している          \n",
    "      def propagate_forward(self, input_datum):\n",
    "        output = input_datum\n",
    "        for layer in self._layers:\n",
    "            output = layer.propagate_forward(output)\n",
    "        return output  \n",
    "        \n",
    "    呼び出された先のlayerインスタンスのpropagate_forward()がなく親クラスを継承しているのでみてみる\n",
    "    以下親クラスのpropagate_forward(x)には入力値(x)を受け取ったらすぐに計算して返している\n",
    "    だから最初の and_gate.propagate_forward([[0,],[0,]])  ここでも入力値のみで値が返されるようになっている\n",
    "    class LogisticLayer(BaseLayer):\n",
    "        name = 'logistic'\n",
    "\n",
    "        def __init__(self, n_output, n_prev_output):\n",
    "            super().__init__(n_output, n_prev_output, Logistic())\n",
    "            \n",
    "    def propagate_forward(self, x):\n",
    "        self._y = self._f.get_value(self._W @ x + self._b)\n",
    "        return self._y            \n",
    "            \n",
    "        \n",
    "\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ---------------------\n",
    "### and_gate.error_func.get_value(np.array([[0, ], [0, ]]), [[0,], ])  \n",
    "### エラー誤差関数の値を取得しているだけ\n",
    "    print(and_gate.error_func.get_value(np.array([[0, ], [0, ]]), [[0,], ]))\n",
    "    入力値の二乗誤差だから0,0 なら必ず０にしかならないし、正解が０なら誤差０なはず\n",
    "    \n",
    "    Neuron.error_funcの中のpropatyを呼び出し\n",
    "    @property\n",
    "    def error_func(self):\n",
    "        return self._error_func\n",
    "    propatyで初期値のerror_funcを呼び出し\n",
    "        \n",
    "    def __init__(self, name, n_input, error_func_name, epsilon):\n",
    "    self._name = name\n",
    "    self._n_input = n_input\n",
    "    self._layers = []\n",
    "    self._error_func = self._get_error_func_by_name(error_func_name)\n",
    "    self._epsilon = epsilon\n",
    "    初期値では設定しているerror_func名を指定して呼んでいる\n",
    "    \n",
    "    呼び出し先メソッドでは定数をループして指定の名前と一致しているもののインスタンスを作っている\n",
    "    _ERROR_FUNCS = [\n",
    "        SquaredError(),\n",
    "        CrossEntropy(),\n",
    "    ]\n",
    "    \n",
    "    def _get_error_func_by_name(self, name):\n",
    "    for func in self._ERROR_FUNCS:\n",
    "        if func.name == name:\n",
    "            return func\n",
    "    raise ErrorFuncDoesNotExist(self._ERROR_FUNCS)\n",
    "    \n",
    "    そのインスタンスのget_value()メソッドを呼び出しているので値が取得されている流れ\n",
    "    最初のコード and_gate.error_func.get_value(np.array([[0, ], [0, ]]), [[0,], ])\n",
    "    error_funcのインスタンスのget_valueでは\n",
    "    正解値(t)と入力値(y)を与えて　二乗誤差を返しているだけ\n",
    "    def get_value(self, t, y):\n",
    "    \"\"\"通常出力\"\"\"\n",
    "    return ((t - y).T @ (t - y)).ravel()[0] / 2.0  # flatten()よりravel()がいい"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ----------------------\n",
    "### and_gate.propagate_backward(np.array([[0, ], [0, ]]), [[1,], ])\n",
    "### 誤差逆伝播については\n",
    "\n",
    "        def propagate_backward(self, input_datum, teaching_datum):\n",
    "            delta = self._error_func.get_derivative(teaching_datum, self.propagate_forward(input_datum))\n",
    "\n",
    "            next_layer = None\n",
    "            for layer in reversed(self._layers):\n",
    "                delta = layer.propagate_backward(delta, next_layer.W if next_layer is not None else None)\n",
    "                next_layer = layer\n",
    "        まずNetworkの呼び出し先をみると引数を２つ用意してあり入力値・正解データを受け取っている\n",
    "        次に、微分値をdeltaで受けているがそこが深くなっているのでしらべてみる\n",
    "        self._error_func.get_derivative()  ではエラー関数の初期値を呼び出しているが最初に設定しており\n",
    "        'logistic'であればSquaredError()のインスタンスが入っているそのインスタンスの\n",
    "        get_derivative()methodを呼び出しているが引数に.get_derivative(teaching_datum, self.propagate_forward(input_datum))\n",
    "        正解データ前後逆だけど、最初に受けとっている正解データを入れて、次に入力値をフォワード関数を呼び出して再度フォワードの出力値を\n",
    "        求めており、求まった出力値を引数に与えている\n",
    "        ようやく get_derivative()メソッドだが、\n",
    "        class SquaredError(Function):\n",
    "           \"\"\"二乗誤差関数\"\"\"\n",
    "            name = \"se\"\n",
    "    \n",
    "            def get_value(self, t, y):\n",
    "            \"\"\"通常出力\"\"\"\n",
    "                return ((t - y).T @ (t - y)).ravel()[0] / 2.0  # flatten()よりravel()がいい\n",
    "   \n",
    "            def get_derivative(self, t, y):\n",
    "                return -(t - y)\n",
    "                \n",
    "            ちゃんとSquaredErrorの微分関数を作って返している\n",
    "            delta = 受け取った微分関数は最初の代入に戻り\n",
    "            \n",
    "            次に\n",
    "            next_layer = None\n",
    "            for layer in reversed(self._layers):\n",
    "            で初期値に持っているlayerを呼び出しながら if next_layer is not Noneでないなら\n",
    "            delta = layer.propagate_backward(delta, next_layer.W)\n",
    "            誤差関数で求めた誤差微分関数を引数に渡して、パラメータとともにそのlayerインスタンスの\n",
    "            propagate_backwardで誤差逆伝播している\n",
    "            今回であれば、'logistic'シグモイド関数レイヤーだけど、\n",
    "            クラスをみると、親クラスに一任しているので親クラスのpropagate_backward()をみてみる\n",
    "            class LogisticLayer(BaseLayer):\n",
    "                name = 'logistic'\n",
    "\n",
    "                def __init__(self, n_output, n_prev_output):\n",
    "                    super().__init__(n_output, n_prev_output, Logistic())\n",
    "             確認すると親クラスでメソッド用意してあり\n",
    "             ２層目か１層目かの確認をしており\n",
    "             今回２層目ならnext_WはNoneなのでelse側へ行き\n",
    "             (delta, next_layer.W)引数で渡している誤差関数微分とそのレイヤーのパラメータを\n",
    "             next_deltaとしてdeltaを渡して、next_layer.Wがないので\n",
    "             self._delta = self._f.get_derivative(self._y) @ next_delta\n",
    "             こちらを処理している\n",
    "             こちらでも,next_deltaの誤差関数微分は使われているがさらに内積している\n",
    "             self._f.get_derivative(self._y) @  \n",
    "             自身が持つ self._f 関数のget_derivativeを読んでいるがこの自身が持つ self._f は\n",
    "             ’logistic'であれば、\n",
    "             class LogisticLayer(BaseLayer):\n",
    "                name = 'logistic'\n",
    "\n",
    "            def __init__(self, n_output, n_prev_output):\n",
    "                super().__init__(n_output, n_prev_output, Logistic())  <--- Logistic()とインスタンスを渡しているのでLogistic()クラスのget_derivative()を呼ぶ\n",
    "                self._yは入力値\n",
    "                \n",
    "            def propagate_backward(self, next_delta, next_W):\n",
    "                if next_W is not None:\n",
    "                    self._delta = self._f.get_derivative(self._y) @ next_W.T @ next_delta\n",
    "                else:\n",
    "                    self._delta = self._f.get_derivative(self._y) @ next_delta\n",
    "                return self._delta      \n",
    "                    \n",
    "              Logistic()がもつget_derivative()は以下\n",
    "                 def get_derivative(self, y):\n",
    "                    assert isinstance(y, np.ndarray) and y.shape[1] == 1\n",
    "                    jacobian = np.zeros(shape=(y.shape[0], y.shape[0]))\n",
    "                    jacobian[np.diag_indices(y.shape[0])] = (y * (1 - y)).ravel()\n",
    "                    return jacobian\n",
    "            ヤコビ行列を求めているので\n",
    "            シグモイド関数の行列を出来上がる\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -------------\n",
    "### and_gate.update(np.array([[0, ], [0, ]]))\n",
    "### 最後にパラメータ調整\n",
    "        \n",
    "        def update(self, input_datum):\n",
    "        prev_layer = None\n",
    "        for layer in self._layers:\n",
    "            layer.update(input_datum if prev_layer is None else prev_layer.y, self._epsilon)\n",
    "            prev_layer = layer\n",
    "        パラメータ調整だけど、入力値をいれると\n",
    "        自身がもつレイヤーを呼び出し\n",
    "        今回では'logistic'だけど、\n",
    "        Logistic()クラスの.updateを呼んでいる、呼び出し時、引数に\n",
    "        もし、次のレイヤーがなければ入力値を与えるがあれば、\n",
    "        そのレイヤーのもつ出力値とハイパーパラメータをupdateへ与える\n",
    "        layer.update(input_datum if prev_layer is None else prev_layer.y, self._epsilon)\n",
    "        \n",
    "        今回'logistic'のみの場合、前の入力値というのは１層でない為\n",
    "        入力値x1,x2が[0,0]として前の入力値 prev_y引数に当たる\n",
    "        以上のことから以下updateメソッドでは以前の入力値とハイパーパラメーたを使う時と\n",
    "        初期入力値とハイパーパラメータを使う場合とある　引数の数が合わなくてもエラーにはならない。\n",
    "        def update(self, prev_y, epsilon):       # 引数でもし前の入力値がない場合、初期入力値 x1,x2が入る\n",
    "            Delta_W = self._delta @ prev_y.T  # これはバックプロップの微分のみ\n",
    "            self._W -= epsilon * Delta_W          # パラメータ調整　W\n",
    "            self._b -= epsilon * self._delta        # パラメータ調整   b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 以下あたら行く与えるデータ\n",
    "\n",
    "えむにストのデータをとってきて\n",
    "一つの配列にして\n",
    "それを投げ込む\n",
    "入力データと正解データがあるので与える\n",
    "- 正解率を表示\n",
    "- ビジュアライズ(学習曲線)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 以下　MPSの更新作業\n",
    "MPS DEEP\n",
    "About\n",
    "\n",
    "A Deep learning library for educational use. Only numpy is used to implement.\n",
    "Supported layers\n",
    "\n",
    "    Logistic sigmoid layer\n",
    "    Tanh layer\n",
    "    Softmax layer\n",
    "    Rectifier layer (ReLU)\n",
    "\n",
    "Supported error functions\n",
    "\n",
    "    Squared error\n",
    "    Cross entropy\n",
    "\n",
    "Other features\n",
    "\n",
    "    Export/Import network structure to/from JSON.\n",
    "- この中に英語を追記していく\n",
    "- 使い方を表記する\n",
    "- まず、Mnistデータをsklearnからimportして　　コードで書く\n",
    "- np.arrayの配列を用意して\n",
    "- 学習回して\n",
    "- 何回回して正解率◯◯を表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
